{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d56b2e",
   "metadata": {},
   "source": [
    "# Construyendo un Motor de Búsqueda desde 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772445f8",
   "metadata": {},
   "source": [
    "Pasos:\n",
    "1. Configurar el ambiente para el desarrollo.\n",
    "   - Configurar ambiente virtual Python.\n",
    "   - Configurar Elasticsearch instance.\n",
    "2. Normalizar contenido para búsqueda. Usaremos una versión reducida de este conjunto de datos disponible públicamente: https://www.kaggle.com/dhruvildave/wikibooks-dataset\n",
    "3. Crear indice Elasticsearch con contenido normalizado.\n",
    "4. Construir sentencias de búsqueda para Elasticsearch.\n",
    "5. Medir el desempeño de mi motor con cada sentencia de búsqueda y escoger la mejor.\n",
    "6. Crear UI y ensamblar todos lo componentes.\n",
    "7. BONUS: Integrar modelos de lenguaje para:\n",
    "    - Mejorar reranking\n",
    "    - Extraer y mostrar respuesta al usuario contenida en los parrafos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7986a5",
   "metadata": {},
   "source": [
    "### 1. Configurar el ambiente para el desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77116228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalar dependencias requeridas\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6045fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniciar elasticsearch instance\n",
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7f20c",
   "metadata": {},
   "source": [
    "### 2. Normalizar contenido para búsqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbdaaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lectura del dataset archivo zip e imprimir numero total de registros\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"dataset/reduced-es-books-dataset.csv.zip\")\n",
    "dataset = dataset[['title', 'url', 'body_html']]\n",
    "print(f\"Total libros: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeaab1e",
   "metadata": {},
   "source": [
    "#### 2.1 Reconocimiento del formato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04973a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir dataset a listado de dicts e imprimir alguno de los elementos\n",
    "books = dataset.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed9864",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([b['title'] for b in books[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540750bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "def display_html(input_html):\n",
    "    display(HTML(input_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ef326",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_html(books[0]['body_html'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d68ff93",
   "metadata": {},
   "source": [
    "#### 2.2 Conversión HTML a formato intermedio Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f1996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML de referencia\n",
    "html = \"<b>Titulo de documento</b>\" + \\\n",
    "\"<br /><br /><br />\" + \\\n",
    "\"Rutas\" + \\\n",
    "\"<ul>\" + \\\n",
    "\"<li><em>Ruta</em> <b>1</b></li>\" + \\\n",
    "\"<li><a href='#example'>Ruta 2</a></li>\" + \\\n",
    "\"</ul>\" + \\\n",
    "\"<a href='https://www.misiontic2022.gov.co/portal/Secciones/Inscripciones-2021/'>Requisitos Ruta 2</a>:\" + \\\n",
    "\"<p>Si eres mayor de 15        años, quieres aprender a programar y tienes la disponibilidad de 30 horas semanales para realizar la formación. Deberás registrate en la web, seleccionar la ruta 2 y diligenciar el formulario de inscripción.</p>\" + \\\n",
    "\"Universidades:\" + \\\n",
    "\"<br />\" + \\\n",
    "\"* UniNorte<br />· UniSimon<br />- Universidad Nacional<br />La universidad nacional ...<br />- UNAB\" + \\\n",
    "\"<br /><br /><br />\" + \\\n",
    "\"<span>[</span><a href='#example' title='Editar sección'>editar</a><span>]</span>Tiempos:\" + \\\n",
    "\"<br />\" + \\\n",
    "\"<table><tr><th>Ruta</th><th>Tiempo</th><th>Edad</th></tr><tr><td>Ruta 1</td><td>11 horas</td><td>12</td></tr><tr><td>Ruta 2</td><td>30 horas</td><td>18</td></tr></table>\" + \\\n",
    "\"<img src='https://blog-assets.lightspeedhq.com/img/2020/04/bbcdda07-xxxxxgoogle-shopping.jpg' alt='mintic 2022' width='500'>\"\n",
    "display_html(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f1135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from html2text import HTML2Text\n",
    "parser = HTML2Text()\n",
    "md = parser.handle(html)\n",
    "print(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb66e85",
   "metadata": {},
   "source": [
    "#### 2.3 Ajustar conversor para lectura óptima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28257b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignorar enlaces, ignorar enfasis, ignorar imagenes y no ajustar ancho de texto\n",
    "from html2text import HTML2Text\n",
    "def convert_from_html_to_md(input_html):\n",
    "    parser = HTML2Text()\n",
    "    parser.ignore_links = True\n",
    "    parser.ignore_emphasis = True\n",
    "    parser.ignore_images = True\n",
    "    parser.body_width = 0\n",
    "    md = parser.handle(input_html)\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf067f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "md = convert_from_html_to_md(html)\n",
    "print(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb5273f",
   "metadata": {},
   "source": [
    "#### 2.4 Normalizar listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a530c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reescribir los elementos de lista usando un correcto formato de Markdown (eg.  * list item) - re.sub\n",
    "import re\n",
    "def normalise_md_lists(input_md):\n",
    "    return re.sub(r'(^|\\n)\\\\?[·*-]( \\w)', r'\\1  *\\2', input_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ece804",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_lists_md = normalise_md_lists(md)\n",
    "print(normalised_lists_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f775a0",
   "metadata": {},
   "source": [
    "#### 2.5 Normalizar tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f6de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. \"| \" -> \" | \"\n",
    "def normalise_md_table(input_md):\n",
    "    normalised = input_md.replace(\"| \", \" | \")\n",
    "    # 2. eliminar \"-----|----|---\"\n",
    "    normalised = re.sub(r'\\-+\\|.*\\n', '\\n', normalised)\n",
    "    # 3. \"    | \" -> \" | \"\n",
    "    normalised = re.sub(r' +\\| +', ' | ', normalised)\n",
    "    return normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b1a016",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_table_md = normalise_md_table(normalised_lists_md)\n",
    "print(normalised_table_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c145d004",
   "metadata": {},
   "source": [
    "#### 2.6 De vuelta a HTML simplificado (MD -> HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f11f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistletoe import markdown\n",
    "def convert_md_to_html(input_md):\n",
    "    return markdown(input_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1d5df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_html = convert_md_to_html(normalised_table_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023445ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_html(simplified_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f302b3",
   "metadata": {},
   "source": [
    "#### 2.7 Conversión HTML a Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89793234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def convert_html_to_text(input_html):\n",
    "    return BeautifulSoup(input_html).getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f209d256",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = convert_html_to_text(simplified_html)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0b3a98",
   "metadata": {},
   "source": [
    "#### 2.8 Manejo extra espacios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cc8f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_spaces(input_text):\n",
    "    # 1. eliminar espacios a los extremos\n",
    "    cleaned = input_text.strip()\n",
    "    # 2. reemplazar tabulaciones y espacios dobles por uno dentro del texto\n",
    "    cleaned = re.sub(r'[\\t ]+', ' ', cleaned)\n",
    "    # 3. reemplazar salto de linea dobles por uno dentro del texto\n",
    "    cleaned = re.sub(r'\\n+', '\\n', cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faedf211",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_extra_spaces_text = clean_up_spaces(text)\n",
    "print(no_extra_spaces_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa04065",
   "metadata": {},
   "source": [
    "#### 2.9 Borrar artefactos innecesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efa387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrar enlaces [editar]\n",
    "def remove_edit_link(input_text):\n",
    "    return input_text.replace(\"[editar]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad23056",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_edit_links_text = remove_edit_link(no_extra_spaces_text)\n",
    "print(no_edit_links_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cee1b0",
   "metadata": {},
   "source": [
    "#### 2.10 Divide el texto en párrafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbefd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paragraphs(input_text):\n",
    "    return input_text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7e6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = extract_paragraphs(no_edit_links_text)\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0671b637",
   "metadata": {},
   "source": [
    "#### 2.11 Filtrar párrafos - definir datos en alcance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fafcd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alcance definido:\n",
    "# 1. solo incluye los párrafos con más de 50 letras\n",
    "def filter_paragraphs(paragraph_list):\n",
    "    filtered_list = [par for par in paragraph_list if len(par) > 50]\n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a5cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_paragraphs = filter_paragraphs(paragraphs)\n",
    "print(filtered_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2354fa9",
   "metadata": {},
   "source": [
    "#### 2.12 Crear función global de normalización de contenido (unir piezas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a910e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_content(input_html):\n",
    "    md = convert_from_html_to_md(input_html)\n",
    "    normalised_lists_md = normalise_md_lists(md)\n",
    "    normalised_table_md = normalise_md_table(normalised_lists_md)\n",
    "    simplified_html = convert_md_to_html(normalised_table_md)\n",
    "    text = convert_html_to_text(simplified_html)\n",
    "    no_extra_spaces_text = clean_up_spaces(text)\n",
    "    no_edit_links_text = remove_edit_link(no_extra_spaces_text)\n",
    "    paragraphs = extract_paragraphs(no_edit_links_text)\n",
    "    filtered_paragraphs = filter_paragraphs(paragraphs)\n",
    "    return filtered_paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed502fdf",
   "metadata": {},
   "source": [
    "#### 2.13 Crear función de normalización de títulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cf28d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikilibros: Wikichicos/La Tierra/Los continentes/Europa/Clima -> La Tierra, Los continentes, Europa, Clima\n",
    "import re\n",
    "def normalise_title(input_title):\n",
    "    clean_title = re.sub(r'Wikilibros|Wikichicos', \"\", input_title)\n",
    "    clean_title = clean_title.strip(\":/ \")\n",
    "    clean_title = \", \".join(clean_title.split(\"/\"))\n",
    "    return clean_title\n",
    "title = \"Wikilibros: Wikichicos/La Tierra/Los continentes/Europa/Clima\"\n",
    "print(normalise_title(title))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b68f81c",
   "metadata": {},
   "source": [
    "#### 2.14 Crear elementos que indexaremos a Elasticsearch - formato definido:\n",
    "{\n",
    "    \"bookId\": 0, \n",
    "    \"bookTitle\": \"\", \n",
    "    \"bookURL\": \"\", \n",
    "    \"paragraphId\": 0,\n",
    "    \"paragraphText\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3984b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "normalized_content = list()\n",
    "for book_id, book in tqdm(enumerate(books)):\n",
    "    paragraphs = normalise_content(book['body_html'])\n",
    "    title = normalise_title(book['title'])\n",
    "    for par_id, par_text in enumerate(paragraphs):\n",
    "        par = {\"bookId\": book_id, \n",
    "               \"bookTitle\": title, \n",
    "               \"bookURL\": book['url'], \n",
    "               \"paragraphId\": f\"{book_id}-{par_id}\",\n",
    "               \"paragraphText\": par_text}\n",
    "        normalized_content.append(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3856545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imprimir total numero de parrafos a almacenar en ES\n",
    "print(f\"Total parrafos: {len(normalized_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ff1618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persistimos contenido normalizado en forma de parrafos al disco duro en formato JSON en dataset/processed/ (crear directorio si no existe)\n",
    "import json\n",
    "import os\n",
    "os.mkdir(\"dataset/processed\")\n",
    "with open(\"dataset/processed/processed.json\", \"w\") as f:\n",
    "    json.dump(normalized_content, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b2cfd",
   "metadata": {},
   "source": [
    "### 3. Crear indice en Elasticsearch con contenido normalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ace3bb",
   "metadata": {},
   "source": [
    "##### 3.1. Instanciamos cliente para poder interactuar con Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a4cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b0f88",
   "metadata": {},
   "source": [
    "##### 3.2. Función para crear índice (indexar contenido) en Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b917b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from elasticsearch import helpers\n",
    "\n",
    "def create_index(index_action_list, index_name):\n",
    "    if es.indices.exists(index=index_name):\n",
    "        print(\"deleting existing elasticsearch index...\")\n",
    "        es.indices.delete(index=index_name, ignore=[400, 404])\n",
    "    \n",
    "    print(\"creating elasticsearch index...\")\n",
    "    request_body = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"bookTitle\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"fields\": {\n",
    "                        \"keyword\": {\n",
    "                            \"type\": \"keyword\",\n",
    "                            \"ignore_above\": 256\n",
    "                        }\n",
    "                    },\n",
    "                    \"analyzer\": \"spanish\"\n",
    "                },\n",
    "                \"paragraphText\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"fields\": {\n",
    "                        \"keyword\": {\n",
    "                            \"type\": \"keyword\",\n",
    "                            \"ignore_above\": 256\n",
    "                        }\n",
    "                    },\n",
    "                    \"analyzer\": \"spanish\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    es.indices.create(index=index_name, body=request_body)\n",
    "\n",
    "    if index_action_list:\n",
    "        print(\"indexing to elasticsearch...\")\n",
    "        for success, info in helpers.parallel_bulk(es, index_action_list, thread_count=os.cpu_count()):\n",
    "            if not success:\n",
    "                print('A document failed:', info)\n",
    "        print(\"done indexing\")\n",
    "    else:\n",
    "        print(\"no items to index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970a84ac",
   "metadata": {},
   "source": [
    "##### 3.4. Creación del índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bccbdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lectura de archivo con parrafos procesados\n",
    "import json\n",
    "with open(\"dataset/processed/processed.json\") as fp:\n",
    "    paragraphs = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186ef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creación de indice con contenido normalizado generado en el paso 2\n",
    "index_name = \"wikibooks-search-index\"\n",
    "action_list = list()\n",
    "for par in normalized_content:\n",
    "    action_list.append({\n",
    "        \"_index\": index_name,\n",
    "        \"_id\": par['paragraphId'],\n",
    "        \"_source\": par\n",
    "    })\n",
    "create_index(action_list, index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa58091",
   "metadata": {},
   "source": [
    "### 4. Construir sentencias de búsqueda (query) para Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de68454",
   "metadata": {},
   "source": [
    "#### 4.1. Funciones para ejecutar búsqueda de contenido en elasticsearch \n",
    "Crear formato a partir de la respuesta de ES y convertir a DF:\n",
    "{\n",
    "    \"bookId\": 0, \n",
    "    \"bookTitle\": \"\", \n",
    "    \"bookURL\": \"\", \n",
    "    \"paragraphId\": 0,\n",
    "    \"paragraphText\": \"\",\n",
    "    \"esScore\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0087510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import helpers\n",
    "from itertools import islice\n",
    "\n",
    "def first_n(iterable, n):\n",
    "    return islice(iterable, 0, n)\n",
    "\n",
    "def parse_to_dataframe(es_candidates):\n",
    "    from collections import defaultdict\n",
    "    import pandas as pd\n",
    "    results = defaultdict(list)\n",
    "    for c in es_candidates:\n",
    "        results['bookTitle'].append(c['_source']['bookTitle'])\n",
    "        results['paragraphText'].append(c['_source']['paragraphText'])\n",
    "        results['esScore'].append(c['_score'])\n",
    "        results['paragraphId'].append(c['_source']['paragraphId'])\n",
    "        results['bookURL'].append(c['_source']['bookURL'])\n",
    "        results['bookId'].append(c['_source']['bookId'])\n",
    "    return pd.DataFrame.from_dict(results)\n",
    "\n",
    "def find_by_query(query, index_name=\"wikibooks-search-index\", size=100):\n",
    "    results = helpers.scan(es, query=query, index=index_name, preserve_order=True)\n",
    "    if size:\n",
    "        results = first_n(results, size)\n",
    "    return parse_to_dataframe(list(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af56676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opción para NO truncar texto cuando es muy largo\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a94e4f",
   "metadata": {},
   "source": [
    "#### 4.2 Demo diferentes tipos de queries soportados por Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2377d1",
   "metadata": {},
   "source": [
    "###### 4.2.1 Query usando un simple campo de búsqueda - paragraphText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e715b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_query_1(user_query):\n",
    "    es_query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\"match\": {\"paragraphText\": user_query}}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return es_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3055887",
   "metadata": {},
   "source": [
    "###### 4.2.2 Query usando múltiples campos de búsqueda - paragraphText y bookTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf7168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_query_2(user_query):\n",
    "    es_query = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": user_query,\n",
    "                \"fields\": [\"paragraphText\", \"bookTitle\"],\n",
    "                \"tie_breaker\": 1.0\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return es_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb36f1a",
   "metadata": {},
   "source": [
    "###### 4.2.3 Query impulsando campos de búsqueda paragraphText y bookTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3b6fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_query_3(user_query):\n",
    "    es_query = {\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": user_query,\n",
    "                \"fields\": [\"paragraphText\", \"bookTitle^2.0\"],\n",
    "                \"tie_breaker\": 1.0\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return es_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f780c2",
   "metadata": {},
   "source": [
    "###### 4.2.4 Query usando múltiples campos de búsqueda con soporte a coincidencia exacta de frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac54107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def es_query_4(user_query):\n",
    "    \n",
    "    match_queries = [\n",
    "        {\"match\": {\"paragraphText\": user_query}},\n",
    "        {\"match\": {\"bookTitle\": user_query}}\n",
    "    ]\n",
    "    \n",
    "    # identificar frases en mi user_query\n",
    "    phrases = re.findall(r'\"([^\"]+)\"', user_query)\n",
    "    # agregar frases a mi ES query\n",
    "    for phrase in phrases:\n",
    "        match_queries.append({\"match\": {\"paragraphText\": phrase}})\n",
    "        match_queries.append({\"match\": {\"bookTitle\": phrase}})\n",
    "    \n",
    "    es_query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": match_queries\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return es_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d64785",
   "metadata": {},
   "source": [
    "### 5. Midiendo el desempeño de mi motor de búsqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4aa70b",
   "metadata": {},
   "source": [
    "##### 5.1 datos preparados para medición de desempeño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784afd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_relevant_answer_pairs = [\n",
    "    {\n",
    "        \"text\": \"extensión total de la Antártida\",\n",
    "        \"relevant_results\": {\n",
    "            \"2803-1\": \"Su extensión total es de aproximadamente 14,2 millones de km2 en verano. Durante el invierno, la Antártida dobla su tamaño a causa de la gran cantidad de hielo marino que se forma en su periferia. El verdadero límite de la Antártida no es el litoral del continente en sí mismo, sino la Convergencia Antártica , que es una zona claramente definida en el extremo sur de los océanos Atlántico, Índico y Pacífico, entre los 48° y los 60° latitud S. En este punto, las corrientes frías que fluyen hacia el Norte desde la Antártida se mezclan con corrientes más cálidas en dirección Sur. La Convergencia Antártica marca una clara diferencia física en los océanos. Por estas razones el agua que rodea al continente antártico se considera un océano en sí mismo, a menudo llamado océano Glacial Antártico o Meridional.\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"dorsales submarinas\",\n",
    "        \"relevant_results\": {\n",
    "            \"3288-15\": \"Las dorsales son cordilleras submarinas formadas al ponerse en contacto el magma del interior de la Tierra con las aguas de los océanos. Por el centro del Atlántico corre una dorsal con forma de S, de unos 15 000 km de longitud, casi paralela a los continentes que da lugar a las islas Azores. El océano Índico también está recorrido por una dorsal de Norte a Sur y en el Pacífico también existen dorsales en dirección Noreste-Sureste; las islas Hawai están situadas sobre una de estas dorsales .\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"principales gases en la atmósfera terrestre\",\n",
    "        \"relevant_results\": {\n",
    "            \"527-12\": \"La atmósfera terrestre es la parte gaseosa de la Tierra, siendo por esto la capa más externa. Está constituida por varios gases que varían en cantidad. Esta mezcla de gases que forma la atmósfera recibe genéricamente el nombre de aire. Los principales elementos que la componen son el oxígeno (21%) y el nitrógeno (78%).\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"anatomía de los mayas\",\n",
    "        \"relevant_results\": {\n",
    "            \"318-10\": \"Eran de baja estatura, hombros anchos, pecho robusto, piernas cortas y musculosas, el rostro alargado y los pómulos salientes. Su estatura era, en término medio, de 1,65 m los varones y 1,42 m las mujeres. El cabello negro y lacio y la piel de color cobrizo confirma su procedencia del norte de Asia.\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"potencia máxima rueda hidráulica\",\n",
    "        \"relevant_results\": {\n",
    "            \"1965-3\": \"Los antiguos aprovechaban ya la energía del agua; utilizaban ruedas hidráulicas para moler trigo. Durante la Edad Media , las enormes ruedas hidráulicas de madera desarrollaban una potencia máxima de cincuenta caballos.\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"¿Cuándo se firmó el tratado antártico?\",\n",
    "        \"relevant_results\": {\n",
    "            \"2803-5\": \"Siete países ( Argentina, Australia, Chile, Francia, Gran Bretaña, Nueva Zelanda y Noruega ) reivindican la soberanía de ciertos territorios de la Antártida, pero desde el Tratado Antártico de 1961 estas demandas han sido abandonadas en favor de la cooperación internacional en las investigaciones científicas.\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"¿Cuánta sal hay en promedio en los océanos?\",\n",
    "        \"relevant_results\": {\n",
    "            \"3288-4\": \"El agua de los océanos contiene una media de 36 gramos por litro de sales. Esto se expresa diciendo que la salinidad media de los océanos es del 36 por 1 000. Entre las sales disueltas, el cloruro de sodio (sal común) es la más abundante. En las costas de las lugares donde hace mucho calor y la evaporación es grande el hombre extrae la sal de las salinas, por ejemplo en el Mediterráneo. también tiene disueltas pequeñas cantidades de yodo, fósforo y cobre.\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"¿Cuántos años tiene la tierra?\",\n",
    "        \"relevant_results\": {\n",
    "            \"2375-0\": \"Hace alrededor de 4550 millones de años atrás se formaron la Tierra y los otros planetas del Sistema Solar a partir de la nebulosa solar; una masa en forma de disco compuesta del polvo y gas que aún quedaba de la formación del Sol. Este proceso de formación de la Tierra tuvo lugar en un plazo de 10-20 millones de años.\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"¿Dónde vivía la abuelita de Caperucita?\",\n",
    "        \"relevant_results\": {\n",
    "            \"3152-2\": \"Caperucita tenía una abuela que vivía en una casita al otro lado del bosque, por lo que para ir a verla tenía que cruzar todo el bosque. En el bosque,vivían animales que no eran peligrosos: como los ciervos, los conejos y muchas especies de pájaros, pero también vivía en ese bosque un animal que sí podía ser peligroso, sobre todo para los niños: era un lobo que cuando tenía mucha hambre atacaba a las personas.\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"¿De qué estaba hecha la casa de Hansel y Gretel?\",\n",
    "        \"relevant_results\": {\n",
    "            \"2997-2\": \"Llegaron a una casita hecha de pan de jengibre, pastel y azúcar moreno,\",\n",
    "            \"2997-4\": \"Después de dos días perdidos en el bosque, cuando ya no sabían más que hacer, los niños se detienen a escuchar el canto de un pájaro blanco al cual luego siguen hasta llegar a una casita hecha de pan de jengibre, pastel y azúcar morena. Hansel y Gretel empezaron a comer, pero lo que no sabían era que esta casita era la trampa de una vieja bruja para encerrarlos y luego comérselos.\"\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc1107",
   "metadata": {},
   "source": [
    "##### 5.2 Funciones para medición de desempeño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(relevant_results, candidates, k):\n",
    "    candidates_in_k = candidates[:min(k, len(candidates))]\n",
    "    tp = len(set(relevant_results).intersection(set(candidates_in_k)))\n",
    "    fn = len(set(relevant_results) - set(candidates_in_k))\n",
    "    return tp / min(k, (tp + fn)) # same as tp / min(k, len(relevant_results))\n",
    "\n",
    "def precision_at_k(relevant_results, candidates, k):\n",
    "    candidates_in_k = candidates[:min(k, len(candidates))]\n",
    "    tp = len(set(relevant_results).intersection(set(candidates_in_k)))\n",
    "    fp = len(set(candidates_in_k) - set(relevant_results))\n",
    "    return tp / min(k, (tp + fp)) # same as tp / min(k, len(candidates_in_k))\n",
    "\n",
    "def average_precision_at_k(relevant_results, candidates, k):\n",
    "    precision_acum = 0\n",
    "    for i in range(k):\n",
    "        if candidates[i] in relevant_results:\n",
    "            rank = i + 1\n",
    "            precision_acum += precision_at_k(relevant_results, candidates, rank)\n",
    "    return precision_acum / min(k, len(relevant_results))\n",
    "\n",
    "def compute_engine_performance(queries_with_relevant_answers, k_list, query_builder_functions):\n",
    "    \"\"\"\n",
    "    Iterar sobre:\n",
    "    - todos los tipos de query de Elasticsearch\n",
    "    - k's = numbero de resultados tope que vamos a cubrir (top 10, 20, etc)\n",
    "    - todos mis resultados relevantes previamente anotados\n",
    "    \n",
    "    Crear un DataFrame con la siguiente información:\n",
    "    - tipo de query\n",
    "    - K\n",
    "    - recall\n",
    "    - average_precision\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from collections import defaultdict\n",
    "    performance_detailed_data = defaultdict(list)\n",
    "    for query_builder in query_builder_functions:\n",
    "        for k in k_list:\n",
    "            for query_answers_pair in queries_with_relevant_answers:\n",
    "                candidates_df = find_by_query(query_builder(query_answers_pair['text']), size=k)\n",
    "                performance_detailed_data['query_builder'].append(query_builder.__name__)\n",
    "                performance_detailed_data['k'].append(k)\n",
    "                performance_detailed_data['query'].append(query_answers_pair['text'])\n",
    "                if not candidates_df.empty:\n",
    "                    ranked_candidates = candidates_df['paragraphId'].to_list()\n",
    "                    relevant_results = list(query_answers_pair['relevant_results'].keys())\n",
    "                    # metrics calculation\n",
    "                    performance_detailed_data['recall'].append(recall_at_k(relevant_results, ranked_candidates, k))\n",
    "                    performance_detailed_data['average_precision'].append(average_precision_at_k(relevant_results, ranked_candidates, k))\n",
    "                else:\n",
    "                    # metrics calculation\n",
    "                    performance_detailed_data['recall'].append(0)\n",
    "                    performance_detailed_data['average_precision'].append(0)\n",
    "    performance_detailed = pd.DataFrame.from_dict(performance_detailed_data)\n",
    "    performance_summary = performance_detailed[['query_builder', 'k', 'recall', 'average_precision']]\n",
    "    performance_summary = performance_summary.groupby(['query_builder', 'k'], as_index=False).agg({\"recall\": \"mean\", \"average_precision\": \"mean\"})\n",
    "    return performance_summary, performance_detailed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c5e5b",
   "metadata": {},
   "source": [
    "##### 5.3 Medición de desempeño de diferentes Elasticsearch queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94155b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_summary_es, _ = compute_engine_performance(query_relevant_answer_pairs, [10, 20], [es_query_1, es_query_2, es_query_3, es_query_4])\n",
    "performance_summary_es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0083988",
   "metadata": {},
   "source": [
    "### 6 Creamos nuestra función de búsqueda final\n",
    "Formato de respuesta esperado:\n",
    "{\n",
    "    \"bookId\": 0, \n",
    "    \"bookTitle\": \"\", \n",
    "    \"bookURL\": \"\", \n",
    "    \"paragraphId\": 0,\n",
    "    \"paragraphText\": \"\",\n",
    "    \"esScore\": 0,\n",
    "    \"questionText\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f532e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_es_response(user_question, es_candidates):\n",
    "    results = list()\n",
    "    for c in es_candidates:\n",
    "        par = dict()\n",
    "        par['questionText'] = user_question\n",
    "        par['bookTitle'] = c['_source']['bookTitle']\n",
    "        par['paragraphText'] = c['_source']['paragraphText']\n",
    "        par['esScore'] = c['_score']\n",
    "        par['paragraphId'] = c['_source']['paragraphId']\n",
    "        par['bookURL'] = c['_source']['bookURL']\n",
    "        par['bookId'] = c['_source']['bookId']\n",
    "        results.append(par)\n",
    "    return results\n",
    "\n",
    "def search_candidates(user_question, index_name=\"wikibooks-search-index\", size=20, es=Elasticsearch()):\n",
    "    match_queries = [\n",
    "        {\"match\": {\"bookTitle\": user_question}},\n",
    "        {\"match\": {\"paragraphText\": user_question}}\n",
    "    ]\n",
    "    phrases = re.findall('\"([^\"]*)\"', user_question)\n",
    "    for phrase in phrases:\n",
    "        match_queries.append({\"match_phrase\": {\"bookTitle\": phrase}})\n",
    "        match_queries.append({\"match_phrase\": {\"paragraphText\": phrase}})\n",
    "\n",
    "    es_query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": match_queries\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    results = helpers.scan(es, query=es_query, index=index_name, preserve_order=True)\n",
    "    results = first_n(results, size)\n",
    "    return format_es_response(user_question, results)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df1cfe4",
   "metadata": {},
   "source": [
    "### 7 BONUS: Integrar modelos de reordenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434411fc",
   "metadata": {},
   "source": [
    "##### 7.1 Funciones para llamado de modelos de reordenamiento (reranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df0597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializamoss los modelos q utilizar\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Sequence Classification Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"amberoad/bert-multilingual-passage-reranking-msmarco\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"amberoad/bert-multilingual-passage-reranking-msmarco\")\n",
    "\n",
    "# Extractive Question Answering Model - for Snippets\n",
    "nlp = pipeline(\n",
    "    'question-answering', \n",
    "    model='mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es',\n",
    "    tokenizer=(\n",
    "        'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es',  \n",
    "        {\"use_fast\": False}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a6070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones utilizadas para normalizar, codificar, y rankear los parrafos usando the Sequence classification model\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def normalize_pairs(pairs): # constants for default input normalization\n",
    "    import re\n",
    "    \n",
    "    QUOTES_TRANSL_TABLE = {ord(x): ord(y) for x, y in zip(u\"‘’´“”\", u\"'''\\\"\\\"\")}\n",
    "    \n",
    "    def _fix_quotes(text):\n",
    "        return text.translate(QUOTES_TRANSL_TABLE)\n",
    "    def _normalize_query(query):\n",
    "        return _fix_quotes(\" \".join(query.split()).lower().replace(\"¿\", \"\").replace(\"?\", \"\"))\n",
    "    def _normalize_paragraph(paragraph):\n",
    "        return _fix_quotes(\" \".join(paragraph.split()))\n",
    "    \n",
    "    return [(_normalize_query(row['questionText']),\n",
    "             _normalize_paragraph(row['paragraphText'])) for row in pairs]\n",
    "        \n",
    "def encode_pairs(pairs_to_encode):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "                normalize_pairs(pairs_to_encode),\n",
    "                max_length=512,\n",
    "                truncation=\"longest_first\",\n",
    "                return_tensors='pt',\n",
    "                padding=\"longest\"\n",
    "            )\n",
    "\n",
    "CPU_BATCH_SIZE = 5\n",
    "def rank_paragraphs(pairs):\n",
    "    pairs.sort(key=lambda x: (len(x[\"questionText\"]) + len(x[\"paragraphText\"])))\n",
    "    result_tensors = list()\n",
    "    for batch in tqdm([pairs[x:x + CPU_BATCH_SIZE] for x in range(0, len(pairs), CPU_BATCH_SIZE)]):\n",
    "        encoded_pairs = encode_pairs(batch)\n",
    "        # use Model to assign sequence (question -> passage) classification score\n",
    "        predicted = model(**encoded_pairs)[0].softmax(dim=1)[:, 1]\n",
    "        result_tensors.append((batch, predicted.to(device=\"cpu\", non_blocking=True)))\n",
    "    \n",
    "    output = list()\n",
    "    for batch, predicted in result_tensors:\n",
    "        extracted_info = predicted.tolist()\n",
    "        for index, score in enumerate(extracted_info):\n",
    "            output.append({\n",
    "                \"questionText\": batch[index][\"questionText\"],\n",
    "                \"bookId\": batch[index][\"bookId\"],\n",
    "                \"bookTitle\": batch[index][\"bookTitle\"],\n",
    "                \"bookURL\": batch[index][\"bookURL\"],\n",
    "                \"paragraphId\": batch[index][\"paragraphId\"],\n",
    "                \"paragraphText\": batch[index][\"paragraphText\"],\n",
    "                \"sequenceScore\": score\n",
    "            })\n",
    "    ranked_paragraphs = sorted(output, key=lambda item: item['sequenceScore'], reverse=True)\n",
    "    return ranked_paragraphs\n",
    "    \n",
    "def get_answers_with_snippets(ranked_paragraphs, snippet_threshold=0.8):\n",
    "    # get top parragraph for each document\n",
    "    top_paragraph_by_book = dict()\n",
    "    for par in ranked_paragraphs:\n",
    "        if par['bookId'] in top_paragraph_by_book:\n",
    "            continue\n",
    "        top_paragraph_by_book[par['bookId']] = par\n",
    "    top_answer_paragraphs = list(top_paragraph_by_book.values())\n",
    "    # assign snippets\n",
    "    for par in tqdm(top_answer_paragraphs):\n",
    "        # use Model to extract answer from paragraph and assign the answer score\n",
    "        if par['sequenceScore'] > snippet_threshold:\n",
    "            extracted_answer = nlp(question=par['questionText'], context=par['paragraphText'])\n",
    "            # print(f\"{par['questionText']} - {extracted_answer['answer']} - {par['sequenceScore']}:{extracted_answer['score']}\")\n",
    "            if extracted_answer['score'] >= snippet_threshold:\n",
    "                par['answerScore'] = extracted_answer['score']\n",
    "                par['answerStart'] = extracted_answer['start']\n",
    "                par['answerEnd'] = extracted_answer['end']\n",
    "                par['answerText'] = extracted_answer['answer']\n",
    "    return top_answer_paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5650db",
   "metadata": {},
   "source": [
    "##### 7.2 Funciones para medición de desempeño con reordenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83d0973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_engine_performance_reranker(queries_with_relevant_answers, k_list, best_es_query_builder, best_es_k):\n",
    "    import pandas as pd\n",
    "    from collections import defaultdict\n",
    "    performance_detailed_data = defaultdict(list)\n",
    "    for query_answers_pair in queries_with_relevant_answers:\n",
    "        candidates_df = find_by_query(best_es_query_builder(query_answers_pair['text']), size=best_es_k)\n",
    "        candidates_df['questionText'] = query_answers_pair['text']\n",
    "        candidates_to_rank = candidates_df.to_dict(\"records\")\n",
    "        relevant_results = list(query_answers_pair['relevant_results'].keys())\n",
    "        ranked_pairs = rank_paragraphs(candidates_to_rank)\n",
    "        ranked_candidates = [p['paragraphId'] for p in ranked_pairs]\n",
    "        for k in k_list:\n",
    "            performance_detailed_data['k'].append(k)\n",
    "            performance_detailed_data['query'].append(query_answers_pair['text'])\n",
    "            # metrics calculation\n",
    "            performance_detailed_data['recall'].append(recall_at_k(relevant_results, ranked_candidates, k))\n",
    "            performance_detailed_data['average_precision'].append(average_precision_at_k(relevant_results, ranked_candidates, k))    \n",
    "    performance_detailed = pd.DataFrame.from_dict(performance_detailed_data)\n",
    "    performance_summary = performance_detailed[['k', 'recall', 'average_precision']]\n",
    "    performance_summary = performance_summary.groupby('k', as_index=False).agg({\"recall\": \"mean\", \"average_precision\": \"mean\"})\n",
    "    return performance_summary, performance_detailed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb53cc6f",
   "metadata": {},
   "source": [
    "##### 7.3 Medición de desempeño de la función de reordenamiento (reranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d061d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_summary_reranker, performance_detailed_reranker = compute_engine_performance_reranker(query_relevant_answer_pairs, [1, 5, 10, 15, 20],  es_query_4, 20)\n",
    "performance_summary_reranker"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
